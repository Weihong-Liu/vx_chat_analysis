# 项目名称：WeChat Intelligence Agent (WIA) - 微信群聊知识情报站
**版本：** v1.0 (MVP)
**核心理念：** 降噪、提纯、知识化。从“看聊天记录”转变为“阅览知识仪表盘”。

---

## 1. 项目背景与目标

### 1.1 背景
微信群聊信息过载，高价值信息（技术文章、工具链接）往往被淹没在闲聊中。现有的总结工具多为“流水账”式摘要，缺乏针对个人关注点（Keywords）的深度筛选。

### 1.2 目标
开发一个基于 LLM Agent 的分析工具，核心流程如下：
1.  **数据源：** 初期读取“昨日聊天记录 JSON”，后期接入“实时 JSON 流”。
2.  **处理核心：** 提取链接 -> 联网爬取内容 -> LLM 总结 -> **基于关键词的权重评分**。
3.  **展示层：** Web 端仪表盘，区分“核心情报”与“普通资讯”，并辅以话题重组和用户画像。

---

## 2. 系统架构与数据流

为了适应从“离线”到“实时”的变更，后端需采用**管道（Pipeline）设计模式**。

### 2.1 数据输入层 (Data Ingestion)
*   **阶段一 (当前)：** `FileLoader`。
    *   手动/定时上传 `chat_history_YYYY-MM-DD.json`。
    *   系统一次性读取并进行批处理。
*   **阶段二 (未来)：** `StreamListener`。
    *   WebSocket 或 WebHook 接收实时推送的单条 JSON 消息。
    *   系统维护一个滑动窗口或队列进行缓冲处理。

### 2.2 数据结构标准
**输入 JSON 示例 (标准格式，无论离线还是实时都转为此格式):**
```json
[
  {
    "msg_id": "123456789",
    "timestamp": 1705881600,
    "sender_id": "wxid_xxxx",
    "sender_name": "江湖的蒙面客",
    "msg_type": 1, // 1文本, 49链接/引用
    "content": "大家看看这个库，专门用来做微信Hook的 https://github.com/example/wx-hook",
    "xml_content": "..." // 如果是卡片消息，会有XML
  }
]
```

---

## 3. 功能需求详解 (Functional Requirements)

### 3.1 核心模块：链接智能捕获与评分 (Link Intelligence Agent)
这是你最核心的差异化功能。

*   **功能流程：**
    1.  **提取 (Extraction)：** 正则匹配消息中的 URL，或解析 XML 卡片消息中的链接。
    2.  **去重 (Deduplication)：** 如果同一天多人转发同一链接，仅处理一次，但记录所有转发者。
    3.  **爬取 (Scraping)：**
        *   Agent 访问链接，获取 Title 和正文 Text。
        *   *兜底策略：* 如果爬取失败（如需要登录），仅使用消息上下文作为分析素材。
    4.  **智能分析 (LLM Processing)：**
        *   **输入：** 网页内容摘要 + 上下文聊天记录 + **用户预设关键词库** (例如：`["LLM", "Agent", "可视化", "Python"]`)。
        *   **输出：**
            *   **摘要：** 一句话总结（50字以内）。
            *   **类别：** 工具 / 教程 / 新闻 / 灌水 / 广告。
            *   **匹配度得分 (0-100)：** 根据内容与关键词的语义相关性打分。
            *   **匹配理由：** 为什么给高分（例如：“提到了 LLM Agent 的具体实现代码”）。

### 3.2 拓展模块：话题重组 (Context Reconstruction)
*   **功能：** 识别多条离散的消息是否属于同一个“讨论串”。
*   **逻辑：**
    *   基于时间窗口（如 5 分钟内）和语义相似度。
    *   提取该话题的“发起人”、“参与者”和“核心结论”。
*   **输出：** 一个包含多条原始消息 ID 的话题包。

### 3.3 拓展模块：用户画像 (User Profiling)
*   **功能：** 基于当日发言生成/更新用户标签。
*   **统计维度：**
    *   **贡献值：** 分享的高分链接数量。
    *   **活跃度：** 发言总数。
    *   **关键词倾向：** 他最常讨论什么？（e.g., 这个人总是发前端相关内容）。

---

## 4. 前端可视化需求 (UI/UX)

网页端不再是一张报纸，而是一个**情报仪表盘 (Dashboard)**。

### 4.1 布局规划
建议采用 **Grid 布局**，分为三个主要区域：

#### A 区：核心情报流 (The Intelligence Feed) - 占据中心最大位置
*   **筛选器：** [只看高分(>80)] [只看工具] [我的关键词]
*   **展示形式（卡片流）：**
    *   **高分卡片 (High-Priority)：** 宽大醒目，绿色/金色边框。
        *   显示：网页标题 + 缩略图。
        *   **AI 批注（重点）：** “此链接匹配你的关键词 [Agent]，这是一个开源 Python 库。”
        *   操作：[一键收藏] [查看原文]
    *   **低分卡片 (Low-Priority)：** 折叠或仅显示单行标题，灰色显示。

#### B 区：话题侧边栏 (Topic Timeline) - 左侧
*   按时间轴显示当天的热门讨论话题。
*   例如：“10:30 - 关于 DeepSeek API 价格的讨论”。
*   点击话题，中间区域弹窗显示该话题下的**纯净版对话记录**（去除插科打诨）。

#### C 区：活跃榜与词云 (Community Stats) - 右侧
*   **用户榜单：** 不是简单的“发言最多”，而是“含金量最高”（分享链接得分最高的用户排前面）。
*   **标签云：** 当日群内提到的高频技术名词。

---

## 5. 开发路线图 (Roadmap)

### 阶段一：MVP (最小可行性产品) - 离线版
1.  **数据：** 手动导入 `data.json`。
2.  **后端：** 实现 Python 脚本，使用 LangChain 调用 LLM。
    *   实现 `LinkExtractor` 和 `Scraper`。
    *   实现 `KeywordMatcher` (Prompt Engineering)。
3.  **前端：** 一个简单的 HTML 生成器或 Streamlit 页面。
    *   只展示链接分析结果（高分/低分）。

### 阶段二：增强版 - 交互与话题
1.  **数据：** 依然离线，但数据结构优化。
2.  **功能：** 加入“话题重组”算法。
3.  **前端：** 切换到 React/Vue 或 Next.js，实现卡片筛选、展开话题详情。

### 阶段三：完全体 - 实时版
1.  **数据：** 搭建 WebSocket 服务，接收实时消息。
2.  **后端：** 引入向量数据库 (Chroma/Milvus) 存储历史记录，支持 RAG 问答。
3.  **功能：** 实时弹窗通知：“监测到一条符合 [可视化] 的高价值链接！”

